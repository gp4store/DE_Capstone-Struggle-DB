Raw Data (S3) → Glue Crawler → Data Catalog
                     ↓
               Glue ETL Jobs (heavy transformations)
                     ↓
Processed Data (S3) → Athena (analytical queries & light transforms)
                     ↓
Final Results (S3) → BI Tools/Reports

SCOPE OF WORK:

0.  Create ERD diagram                                                                          PENDING
1.  Flow chart with AWS services logos                                                          OK
2.  Buckets creation throgh a python Script                                                     OK                             
    -   Raw data bucket and process data bucket                                                 OK
3.  Insert null data to csvs to make it more real                                               OK
    -   Easiest method thru a python script                                                     OK
4.  Upload .csv files to raw data bucket from console no need to use script                     OK
    - For general information how apps upload raw data files to an AWS bucket                   PENDING
5.  Set up an IAM role for glue and athena permissions                                          OK
6.  Create AWS glue database from python script                                                 OK
6.  Set up glue crawler through a python script                                                 OK
7.  Consume the data catalog using an aws glue etl jobs                                         OK
8.  Set up a glue etl job thru script                                                           OK
9.  Target clean data bucket from glue etl job                                                  OK
10. Wrap everything on a jupyter notebook   

---------------------------- 06052025 ------------------------------------------------------------------------------------

This is an Extended ETL pipeline with multi-stage processing - specifically a Data Lake Analytics Pipeline with Query-Based ETL.
Pipeline Classification:
Hybrid ETL/ELT Pattern: You have both ETL (Glue transformations) and ELT (Athena queries on already-loaded data) components.
Multi-Hop Architecture: Data flows through multiple S3 locations, with each stage serving different purposes.
Detailed Flow:

Extract: Raw data from S3 (Bronze layer)
Transform: Glue processes and cleans data

Crawler discovers schemas → Data Catalog
Glue jobs perform transformations


Load: Processed data to S3 (Silver layer)
Analyze: Athena queries the processed data
Load Again: Query results to S3 (Gold layer)

This Creates a Medallion Architecture:
Bronze Layer (S3 #1): Raw, unprocessed data
Silver Layer (S3 #2): Cleaned, transformed data from Glue
Gold Layer (S3 #3): Aggregated, business-ready data from Athena queries
Key Characteristics:
Data Lineage: Clear progression from raw → processed → analytical datasets
Separation of Concerns: Glue handles complex ETL, Athena handles analytical transformations
Reusability: Silver layer can be queried for different analytical purposes
Scalability: Each stage can be optimized independently
Common Use Cases:

Silver Layer: Standardized, clean data for multiple analytics teams
Gold Layer: Specific business metrics, KPIs, or reports
Incremental Processing: Only reprocess what's needed at each stage

This pattern is excellent for organizations that need both robust data processing (Glue) and flexible analytical 
capabilities (Athena), with clear data quality progression through each layer.